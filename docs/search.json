[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "",
    "section": "",
    "text": "Use this repository to host a website for your CASA0025 final project by following these stpes:\n\nclone this repository\ninstall quarto\nedit the ‘index.qmd’ file with the contents of your project\nusing terminal, navigate to the project directory and run “quarto render”\npush the changes to your github repository\non github, navigate to Settings&gt;Pages&gt;Build and Deployment. Make sure that under “Source” it says “deploy from branch”. Under “Branch”, select “Main” in the first dropdown and “Docs” under the second drop down. Then press “Save”\n\nYour website should now be available under https://{your_username}.github.io/{your_repo_name}"
  },
  {
    "objectID": "CASA0025_Project_Template/readme.html",
    "href": "CASA0025_Project_Template/readme.html",
    "title": "",
    "section": "",
    "text": "Use this repository to host a website for your CASA0025 final project by following these stpes:\n\nclone this repository\ninstall quarto\nedit the ‘index.qmd’ file with the contents of your project\nusing terminal, navigate to the project directory and run “quarto render”\npush the changes to your github repository\non github, navigate to Settings&gt;Pages&gt;Build and Deployment. Make sure that under “Source” it says “deploy from branch”. Under “Branch”, select “Main” in the first dropdown and “Docs” under the second drop down. Then press “Save”\n\nYour website should now be available under (https://mengyuanhan1.github.io/BigData/)"
  },
  {
    "objectID": "CASA0025_Project_Template/index.html",
    "href": "CASA0025_Project_Template/index.html",
    "title": "CASA00025 Group Project Title Here",
    "section": "",
    "text": "This project crafts an application within Google Earth Engine to evaluate the repercussions of the substantial flood event that transpired in Al Marj, Libya, during September 2023. The application integrates satellite imagery for flood mapping, land cover data for analyzing affected land cover types, and random forest classification to detect affected buildings. Additionally, it utilizes population grid data to estimate the number of people affected by the flood. The results provide a comprehensive assessment of the flood extent, affected land cover areas, impacted buildings, and the affected population, which is crucial for informing disaster response and management efforts.\n\n\nIn September 2023, severe flooding in Al Marj, Libya, caused significant damage to infrastructure and displaced numerous people. Rapid and accurate assessment of the flood’s impact is essential for effective disaster response and management. By leveraging remote sensing data and population grid information, this application aims to provide a comprehensive assessment of various aspects of the flood’s impact. This enables emergency responders to identify and reach the most severely affected areas, and allows governments or international organizations to plan for long-term recovery and infrastructure restoration. Moreover, the framework of this model can be applied to other regions facing similar challenges.\n\n\n\nGovernment departments and international organizations:\nBy visualizing flooded areas, affected buildings, and population data, our project enables these departments to quickly and accurately assess the disaster situation. This facilitates effective rescue and reconstruction planning, as well as optimization of future urban development, land use, and drainage systems.\nInsurance companies:\nOur project supports insurance companies by estimating the area of affected buildings, allowing them to accurately assess economic losses and settle claims promptly. This data-driven approach streamlines the claim settlement process and ensures fair compensation for those affected by flood disasters.\n\n\n\nSentinel-1 Satellite Image: Sentinel-1 collects C-band synthetic aperture radar (SAR) imagery at a variety of polarizations and resolutions. We use it to Detect flood coverage.\nSentinel-2 Satellite Image: Sentinel-2 is a wide-swath, high-resolution, multispectral imaging mission with a global 5-day revisit frequency.\nLand Classification: ESA WorldCover 10m v200 ESA WorldCover10m contains 11 land cover classes based on Sentinel-1 and Sentinel-2 data.\nPopulation: GHSL: Global population surfaces 1975-2030 contains the spatial distribution of residential population, expressed as the absolute number of inhabitants of the cell.\n\n\n\nFlood extent mapping: \nThe areas affected by the flood are identified by calculating the difference between pre-flood and post-flood images.\nLand cover impact assessment: By overlaying the flood extent layer with the land cover, the affected built-up areas and croplands are assessed.\nBuilding damage assessment: \nThe random forest classification algorithm is applied to the Sentinel-2 multispectral imagery to extract buildings within the built-up areas. The accuracy is evaluated using a confusion matrix.\nPopulation impact assessment: \nBy overlaying the flood extent map with the population grid, the application calculates the total number of individuals residing within the inundated areas.\n\n\n\nUsers will be able to interact with the map and perform the following actions:\n\nDraw an Area of Interest (AOI): Users can draw a polygon on the map to define a specific area they want to analyze.\nObtain flood impact information: the application will process the data within the selected area and display the key indicators in the results panel, including Estimated flood extent, Damaged Built-Up Areas, Damaged Crop Land Areas, Estimated Damaged Buildings Land Area, Estimated Affected Population.\nVisualization: The map will display the corresponding visual representations of this information within the AOI.\n\n\n\n\n\nThis is our Earth Engine application.\n\n\n\n\n\n\n\n\n\n1. Flood Area Analysis\n1.1 Data Preprocessing\nWe imported the administrative boundary data of Al Marj region and filtered the area of interest (AOI).\n// Import the administrative boundary of Libya and filter the target region (Al Marj) as the area of interest (AOI)\nvar admin = ee.FeatureCollection(\"projects/ee-hanmengyuan9/assets/libya\");\nvar geometry = admin.filter(ee.Filter.eq('shapeName', 'Al Marj'));\nMap.addLayer(geometry, {color: 'grey'}, 'Al Marj');\nAnd we selected the Sentinel-1 SAR image collection, set parameters and filter images for the flood periods.\n// Define the time periods before and after the flood event\nvar before_start = '2023-09-01';\nvar before_end = '2023-09-15';\nvar after_start = '2023-09-23';\nvar after_end = '2023-09-30';\n\n// Set sensor parameters for the satellite data collection\nvar polarization = (\"VH\", \"VV\");\nvar pass_direction = \"DESCENDING\";\nvar difference_threshold = 1.00;\n\n// Defining the area of interest based on the filtered geometry\nvar aoi = geometry;\n\n// Filtering the satellite image collection based on the specified parameters\nvar collection = ee.ImageCollection('COPERNICUS/S1_GRD')\n  .filter(ee.Filter.eq('instrumentMode', 'IW'))\n  .filter(ee.Filter.listContains('transmitterReceiverPolarisation', polarization))\n  .filter(ee.Filter.eq('orbitProperties_pass', pass_direction))\n  .filter(ee.Filter.eq('resolution_meters', 10))\n  .filterBounds(aoi)\n  .select(polarization);\n  \n// Filter the image collection to obtain images from before and after the flood\nvar before_collection = collection.filterDate(before_start, before_end);\nvar after_collection = collection.filterDate(after_start, after_end);\nThen, we mosaiced, clipped, and applied speckle noise reduction to the filtered before and after image collections to optimize data quality.\n// Create a mosaic of selected tiles and clip to study area\nvar before = before_collection.mosaic().clip(aoi);\nvar after = after_collection.mosaic().clip(aoi);\n\n// Apply reduce the radar speckle by smoothing  \nvar smoothing_radius = 50;\nvar before_filtered = before.focal_mean(smoothing_radius, 'circle', 'meters');\nvar after_filtered = after.focal_mean(smoothing_radius, 'circle', 'meters');\n1.2 Flood Extent Extraction\nWe extracted the flood extent using threshold segmentation and further optimize the flood extent using surface water data, connectivity analysis, and terrain filtering.\n// Calculate the difference between the before and after images\nvar difference = after_filtered.divide(before_filtered);\n\n// Apply the predefined difference-threshold and create the flood extent mask \nvar threshold = difference_threshold;\nvar difference_binary = difference.gt(threshold);\n\n// Refine flood result using additional datasets\n      \n      // Include JRC layer on surface water seasonality to mask flood pixels from areas\n      // of \"permanent\" water (where there is water &gt; 10 months of the year)\n      var swater = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('seasonality');\n      var swater_mask = swater.gte(10).updateMask(swater.gte(10));\n      \n      // Flooded layer where perennial water bodies (water &gt; 10 mo/yr) is assigned a 0 value\n      var flooded_mask = difference_binary.where(swater_mask,0);\n      // final flooded area without pixels in perennial waterbodies\n      var flooded = flooded_mask.updateMask(flooded_mask);\n      \n      // Compute connectivity of pixels to eliminate those connected to 8 or fewer neighbours\n      // This operation reduces noise of the flood extent product \n      var connections = flooded.connectedPixelCount();    \n      var flooded = flooded.updateMask(connections.gte(8));\n      \n      // Mask out areas with more than 5 percent slope using a Digital Elevation Model \n      var DEM = ee.Image('WWF/HydroSHEDS/03VFDEM');\n      var terrain = ee.Algorithms.Terrain(DEM);\n      var slope = terrain.select('slope');\n      var flooded = flooded.updateMask(slope.lt(5));\n1.3 Flood Area Calculation\nWe calculated the flood inundation area based on pixel area and regional statistics.\n// Calculate flood extent area\n// Create a raster layer containing the area information of each pixel \nvar flood_pixelarea = flooded.select(polarization)\n  .multiply(ee.Image.pixelArea());\n\n// Sum the areas of flooded pixels\n// default is set to 'bestEffort: true' in order to reduce computation time, for a more \n// accurate result set bestEffort to false and increase 'maxPixels'. \nvar flood_stats = flood_pixelarea.reduceRegion({\n  reducer: ee.Reducer.sum(),              \n  geometry: aoi,\n  scale: 10, // native resolution \n  //maxPixels: 1e9,\n  bestEffort: true\n  });\n\n// Convert the flood extent to hectares (area calculations are originally given in meters)  \nvar flood_area_ha = flood_stats\n  .getNumber(polarization)\n  .divide(10000)\n  .round();\n2. Identify buildings affected by flooding\n2.1 Data preprocessing\nThe obtained Sentinel-2 image is filtered, processed, cropped, and a preliminary image is generated. A mask is then applied to the data based on NDWI and NDVI thresholds (0.3 and 0.2, respectively) to filter out non-water and low vegetation cover areas.\n// Define an array of band names for Sentinel-2 imagery\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'];\n\n// Create an ImageCollection of Sentinel-2 surface reflectance images\nvar sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR')\n  .filter(ee.Filter.date(before_start, after_end)) // Filter images by date range\n  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) // Filter images with less than 10% cloud cover\n  .mean() // Compute the mean value of each pixel across the filtered images\n  .select(bands) // Select the specified bands\n  .clip(geometry); // Clip the resulting image to the specified geometry\n\n// Define visualization parameters for RGB composite\nvar s_rgb = {\n  min: 0.0, // Minimum value for stretching the image\n  max: 3000, // Maximum value for stretching the image\n  bands: ['B4', 'B3', 'B2'], // Red, Green, Blue bands for the composite\n  opacity: 1 // Opacity of the layer\n};\n\n// Calculate NDVI (Normalized Difference Vegetation Index)\nvar ndvi = sentinel2.normalizedDifference(['B8', 'B4']).select(['nd'], ['ndvi']);\n\n// Calculate NDWI (Normalized Difference Water Index)\nvar ndwi = sentinel2.normalizedDifference(['B3', 'B8']).select(['nd'], ['ndwi']);\n\n// Create a masked image based on NDWI and NDVI thresholds and add NDVI band\nvar image = sentinel2\n  .updateMask(ndwi.lt(0.3)) // Mask pixels with NDWI less than 0.3\n  .updateMask(ndvi.lt(0.2)) // Mask pixels with NDVI less than 0.2\n  .addBands(ndvi); // Add the NDVI band to the image\n2.2 RF analysis\nTo improve the accuracy of the training building model, we will classify and identify different features (such as buildings, farmland, deserts, etc.) together by Random Forest.\nvar building_points = ee.FeatureCollection.randomPoints(buildings2, 3000).map(function(i) {\n  return i.set({'class': 0});\n});\nvar farm_points = ee.FeatureCollection.randomPoints(farmland, 3000).map(function(i) {\n  return i.set({'class': 1});\n});\nvar desert_points = ee.FeatureCollection.randomPoints(desert, 3000).map(function(i) {\n  return i.set({'class': 2});\n});\nvar water_points = ee.FeatureCollection.randomPoints(water, 3000).map(function(i) {\n  return i.set({'class': 3});\n});\nvar road_points = ee.FeatureCollection.randomPoints(road, 3000).map(function(i) {\n  return i.set({'class': 4});\n});\nvar parking_points = ee.FeatureCollection.randomPoints(parking, 3000).map(function(i) {\n  return i.set({'class': 5});\n});\nCombine the random points and split them into training and validation sets and we used the split ratio equal to 0.7.\n// Combine the random points for each land cover class into a single FeatureCollection\nvar sample = ee.FeatureCollection([\n  building_points,\n  farm_points,\n  desert_points,\n  water_points,\n  road_points,\n  parking_points\n])\n.flatten()  // Flatten the FeatureCollection\n.randomColumn(); // Add a random column to the FeatureCollection for splitting\n\n// Define the split ratio for training and validation samples\nvar split = 0.7;\n\n// Create training sample by filtering the sample FeatureCollection where the random column is less than the split ratio\nvar training_sample = sample.filter(ee.Filter.lt('random', split));\n\n// Create validation sample by filtering the sample FeatureCollection where the random column is greater than or equal to the split ratio\nvar validation_sample = sample.filter(ee.Filter.gte('random', split));\nSample the image using the training and validation points and Train a Random Forest classifier using the training samples.\n// Sample the image using the training sample\nvar training = image.sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\n// Sample the image using the validation sample\nvar validation = image.sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Train a Random Forest classifier using the training samples\nvar model = ee.Classifier.smileRandomForest(400)\n  .train(training, 'class');\nClassify the image using the trained model and extract the building class from the prediction.\n// Classify the image using the trained model\nvar prediction = image.classify(model);\n\n// Extract the building class from the prediction by masking pixels that are not classified as buildings (class 0)\nvar building_prediction = prediction.updateMask(prediction.eq(0));\n2.3 Calculate the built-up areas, croplands, buildings footprint and population after the disaster\nImport ESA WorldCover into the map and extract cropland and built-up land from it. By intersecting the Build-up land with the identified buildings, we will focus on the disaster situation of the buildings in the built-up area. Finally, the flood data mask was used to obtain the cropland, built-up areas, building footprint and population after the disaster.\n// Use the ESA WorldCover 10m v200 dataset\nvar dataset = ee.ImageCollection('ESA/WorldCover/v200')\n  .first() // Take the first Image\n  .select('Map') // Select the 'Map' band\n  .clip(aoi);\n\n// Get the projection information of ESA WorldCover\nvar worldCoverProjection = dataset.projection();\n\n// Reproject the flood layer to the scale of ESA WorldCover\nvar flooded_res = flooded.reproject({\n  crs: worldCoverProjection\n});\n\n// Add the ESA WorldCover layer to the map\nvar worldCoverVis = {\n  min: 10,\n  max: 100,\n  palette: [\n    '#006400', '#ffbb22', '#ffff4c', '#f096ff', '#fa0000',\n    '#b4b4b4', '#f0f0f0', '#0064c8', '#0096a0', '#00cf75',\n    '#fae6a0'\n  ]\n};\nMap.addLayer(dataset, worldCoverVis, 'ESA WorldCover');\n\n// Load the dataset\nvar worldCover = ee.ImageCollection('ESA/WorldCover/v200').first().clip(geometry);\nvar builtUp = worldCover.updateMask(worldCover.eq(50));\nvar builtUpVisParams = {\n  palette: ['ff0000'],\n  opacity: 0.7\n};\n\n// Add built-up area layer to the map\nMap.addLayer(builtUp, builtUpVisParams, 'Built-up Area');\n\n// Select cropland category (class code 40 corresponds to Rain-fed cropland)\nvar cropland = dataset.eq(40);\n\n// Calculate affected cropland area\nvar croplandAffected = flooded_res.updateMask(cropland);\n\n// Select built-up category (class code 50)\nvar buildland = dataset.eq(50);\nvar class_prediction = prediction.updateMask(prediction.eq(0));\n\n// Mask the classification result using the built-up area image\nvar maskedClassPrediction = class_prediction.updateMask(builtUp);\n\n// Add the masked classification result to the map\nMap.addLayer(maskedClassPrediction, {palette:'red'}, 'Built-up Area Prediction');\n\n// ---------- Calculate flood-affected population -----------\n\n// Load population data from GHSL\nvar population = ee.Image('JRC/GHSL/P2023A/GHS_POP/2020')\n  .clip(aoi);\n\n// Get GHSL projection\nvar GHSLprojection = population.projection();\n\n// Reproject flood layer to GHSL scale\nvar flooded_res1 = flooded.reproject({\n  crs: GHSLprojection\n});\n\n// Calculate flood-affected population by applying the flood and population masks\nvar floodedPopulation = population\n  .updateMask(flooded_res1)\n  .updateMask(population);\nCalculate the area of affected croplands, built-up areas, building footprint, and population.\n// ------------------ Cropland ------------\n// Calculate the area of each affected cropland pixel\nvar affectedCroplandArea = Affected_Cropland.multiply(ee.Image.pixelArea());\n\n// Sum the area of affected cropland\nvar croplandStats = affectedCroplandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n}); \n\nvar croplandAreaHectares = ee.Number(croplandStats.get('b1')).divide(10000).round();\n// Calculate the area of each affected pixel in the built-up area\nvar affectedBuildlandArea = Affected_Builtup.multiply(ee.Image.pixelArea());\n\n// -------------builtup area ----------------\n\n// Sum the area of the affected built-up area\nvar buildlandStats = affectedBuildlandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n});\n\nvar buildlandAreaHectares = ee.Number(buildlandStats.get('b1')).divide(10000).round();\n\n// ----------   population ----------- \n\nvar populationStats = Flooded_Population.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 100, \n  maxPixels: 1e9\n  //bestEffort: true\n});\n\nvar affectedPopulation = ee.Number(populationStats.get('b1')).round();\n\n// ----------- buildings footprint -------------\n\nvar buildingFootprintArea = buildingsFootprint.multiply(ee.Image.pixelArea());\nvar buildingFootprintAreaSum = buildingFootprintArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, \n  maxPixels: 1e13\n});\n\nvar buildingAreaHectares = ee.Number(buildingFootprintAreaSum.get('b1')).divide(10000).round();\n\n\n//  -------------- output ------------\nprint(croplandAreaHectares);\nprint(buildlandAreaHectares);\nprint(affectedPopulation);\nprint(buildingAreaHectares);\n\n\n\n1.Create a map instance and Set the center point and zoom level of the map\nvar map = ui.Map();\nmap.centerObject(aoi,10.5);\n\nThe results of the data volt analysis are displayed in the lower right corner of the user interface. This user interface features a panel with a series of informational sections and a legend, organized as follows:\n\nResults Section: At the top of the panel, there is a “Results” header indicating the category of information provided. Below the header, several pieces of information are listed: - Flood Status: Dates between which the flood status is reported (2023-09-23 to 2023-09-30). - Estimated Flood Extent: The area affected by the flood, as estimated from Sentinel-1 imagery, is listed as 1023443 hectares. - Damaged Built-Up Areas: An estimate of the damaged urban area is provided (2943 hectares). - Damaged Crop Land Areas: The extent of the damage to cropland is reported (142837 hectares). - Estimated Damaged Buildings Land Area: This section lists the estimated area of damaged buildings (367 hectares). - Estimated Affected Population: The number of people estimated to be affected by the event (200094).\nLegend: Below the results and separated by a horizontal line, there is a legend that visually correlates colors with types of areas or statuses: - A blue square represents areas affected by the flood. - A red square denotes built-up areas. - A purple square indicates cropland. - A darker purple square is used for damaged buildings.\nEach legend item consists of a colored square followed by a text label that describes what the color represents in the context of the map being displayed.\n// Create a vertical panel that acts as a container for the entire right panel\nvar rightPanel = ui.Panel({\n  style: {\n    position: 'bottom-right',\n    padding: '8px 15px',\n    width: '250px' // Adjust the width as needed\n  },\n  layout: ui.Panel.Layout.flow('vertical') // Set the panel vertical layout\n});\n\n// Create a panel to display the results\nvar results = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0 0 8px 0' // Add a bottom margin to separate it from the legend panel\n  }\n});\n\n// Prepare the visualization parameters of the labels\n// Defines visual style parameters for text labels\nvar textVis = {\n  'margin':'0px 8px 2px 0px',\n  'fontWeight':'bold'\n};\nvar numberVIS = {\n  'margin':'0px 0px 15px 0px', \n  'color':'bf0f19',\n  'fontWeight':'bold'\n};\nvar subTextVis = {\n  'margin':'0px 0px 2px 0px',\n  'fontSize':'12px',\n  'color':'grey'\n};\n\nvar titleTextVis = {\n  'margin':'0px 0px 15px 0px',\n  'fontSize': '18px', \n  'color': '3333ff'\n};\n\n// Create text labels for titles and data\nvar title = ui.Label('Results', titleTextVis);\nvar text1 = ui.Label('Flood status between:', textVis);\nvar number1 = ui.Label(after_start.concat(\" and \",after_end), numberVIS);\n\n// The default text label is \"Please wait...\" Then replace it with the actual data\nvar text2 = ui.Label('Estimated flood extent:', textVis);\nvar text2_2 = ui.Label('Please wait...', subTextVis);\ndates(after_collection).evaluate(function(val){text2_2.setValue('based on Sentinel-1 imagery '+val)});\nvar number2 = ui.Label('Please wait...', numberVIS); \nflood_area_ha.evaluate(function(val){number2.setValue(val+' hectares')});\n\n//some more data\nvar text3 = ui.Label('Damaged Built-Up Areas (Ha):', textVis);\nvar number3 = ui.Label('Please wait...', numberVIS);\ndamagedBuildUpAreas.evaluate(function(val) {\n  number3.setValue(val + ' hectares');\n});\n\nvar text4 = ui.Label('Damaged Crop Land Areas (Ha):', textVis);\nvar number4 = ui.Label('Please wait...', numberVIS);\ndamagedCropLandAreas.evaluate(function(val) {\n  number4.setValue(val + ' hectares');\n});\n\nvar text5 = ui.Label('Estimated Damaged Buildings Land Area (Ha):', textVis);\nvar number5 = ui.Label('Please wait...', numberVIS);\nestimatedDamagedBuildings.evaluate(function(val) {\n  number5.setValue(val + ' hectares');\n});\n\nvar text6 = ui.Label('Estimated Affected Population:', textVis);\nvar number6 = ui.Label('Please wait...', numberVIS);\nestimatedAffectedPopulation.evaluate(function(val) {\n  number6.setValue(val.toString());\n});\n\nresults.add(ui.Panel([\n        title,\n        text1,\n        number1,\n        text2,\n        text2_2,\n        number2,\n        text3,\n        number3,\n        text4,\n        number4,\n        text5,\n        number5,\n        text6,\n        number6,\n       ]\n      ));\n      \n      \n// Create a function to generate legend items with colors and labels\nfunction createLegendItem(color, label) {\n  return ui.Panel({\n    widgets: [\n      ui.Label('', {\n        backgroundColor: color,\n        padding: '8px', // Adjust to fit the size of the legend color block\n        margin: '0 4px 0 0',\n      }),\n      ui.Label(label, {\n        margin: '-8px 0 0 4px',\n        padding: '8px 0px', // Adjust the upper and lower margins of the text to align it vertically\n        fontSize: '12px'\n      })\n    ],\n    layout: ui.Panel.Layout.Flow('horizontal')\n  });\n}\n\n\n// Create a legend panel and add a title\nvar legend = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0'\n  }\n});\nlegend.add(ui.Label('Legend', {fontWeight: 'bold', fontSize: '16px', margin: '0 0 4px 0'}));\n\n// Create a legend item using a function and add it to the legend panel\nlegend.add(createLegendItem('blue', 'Flood'));\nlegend.add(createLegendItem('#fa0000', 'Built-up'));\nlegend.add(createLegendItem('#f096ff', 'CropLand'));\nlegend.add(createLegendItem('purple', 'Damaged Building'));\n\n// Add a splitter between the results panel and the Legend panel\nvar separatorLine = ui.Panel({\n  style: {\n    height: '2px',\n    backgroundColor: 'black',\n    margin: '8px 0'\n  }\n});\n\n// Add the results panel and Legend panel to the right panel container\nrightPanel.add(results);\nrightPanel.add(separatorLine); \nrightPanel.add(legend);\n\n// Adds the entire right panel to the map interface\nMap.add(rightPanel);\n\n\n\n\nhttps://github.com/MengyuanHan1/BigData/tree/main"
  },
  {
    "objectID": "CASA0025_Project_Template/index.html#project-summary",
    "href": "CASA0025_Project_Template/index.html#project-summary",
    "title": "CASA00025 Group Project Title Here",
    "section": "",
    "text": "This project crafts an application within Google Earth Engine to evaluate the repercussions of the substantial flood event that transpired in Al Marj, Libya, during September 2023. The application integrates satellite imagery for flood mapping, land cover data for analyzing affected land cover types, and random forest classification to detect affected buildings. Additionally, it utilizes population grid data to estimate the number of people affected by the flood. The results provide a comprehensive assessment of the flood extent, affected land cover areas, impacted buildings, and the affected population, which is crucial for informing disaster response and management efforts.\n\n\nIn September 2023, severe flooding in Al Marj, Libya, caused significant damage to infrastructure and displaced numerous people. Rapid and accurate assessment of the flood’s impact is essential for effective disaster response and management. By leveraging remote sensing data and population grid information, this application aims to provide a comprehensive assessment of various aspects of the flood’s impact. This enables emergency responders to identify and reach the most severely affected areas, and allows governments or international organizations to plan for long-term recovery and infrastructure restoration. Moreover, the framework of this model can be applied to other regions facing similar challenges.\n\n\n\nGovernment departments and international organizations:\nBy visualizing flooded areas, affected buildings, and population data, our project enables these departments to quickly and accurately assess the disaster situation. This facilitates effective rescue and reconstruction planning, as well as optimization of future urban development, land use, and drainage systems.\nInsurance companies:\nOur project supports insurance companies by estimating the area of affected buildings, allowing them to accurately assess economic losses and settle claims promptly. This data-driven approach streamlines the claim settlement process and ensures fair compensation for those affected by flood disasters.\n\n\n\nSentinel-1 Satellite Image: Sentinel-1 collects C-band synthetic aperture radar (SAR) imagery at a variety of polarizations and resolutions. We use it to Detect flood coverage.\nSentinel-2 Satellite Image: Sentinel-2 is a wide-swath, high-resolution, multispectral imaging mission with a global 5-day revisit frequency.\nLand Classification: ESA WorldCover 10m v200 ESA WorldCover10m contains 11 land cover classes based on Sentinel-1 and Sentinel-2 data.\nPopulation: GHSL: Global population surfaces 1975-2030 contains the spatial distribution of residential population, expressed as the absolute number of inhabitants of the cell.\n\n\n\nFlood extent mapping: \nThe areas affected by the flood are identified by calculating the difference between pre-flood and post-flood images.\nLand cover impact assessment: By overlaying the flood extent layer with the land cover, the affected built-up areas and croplands are assessed.\nBuilding damage assessment: \nThe random forest classification algorithm is applied to the Sentinel-2 multispectral imagery to extract buildings within the built-up areas. The accuracy is evaluated using a confusion matrix.\nPopulation impact assessment: \nBy overlaying the flood extent map with the population grid, the application calculates the total number of individuals residing within the inundated areas.\n\n\n\nUsers will be able to interact with the map and perform the following actions:\n\nDraw an Area of Interest (AOI): Users can draw a polygon on the map to define a specific area they want to analyze.\nObtain flood impact information: the application will process the data within the selected area and display the key indicators in the results panel, including Estimated flood extent, Damaged Built-Up Areas, Damaged Crop Land Areas, Estimated Damaged Buildings Land Area, Estimated Affected Population.\nVisualization: The map will display the corresponding visual representations of this information within the AOI."
  },
  {
    "objectID": "CASA0025_Project_Template/index.html#the-application",
    "href": "CASA0025_Project_Template/index.html#the-application",
    "title": "CASA00025 Group Project Title Here",
    "section": "",
    "text": "This is our Earth Engine application."
  },
  {
    "objectID": "CASA0025_Project_Template/index.html#how-it-works",
    "href": "CASA0025_Project_Template/index.html#how-it-works",
    "title": "CASA00025 Group Project Title Here",
    "section": "",
    "text": "1. Flood Area Analysis\n1.1 Data Preprocessing\nWe imported the administrative boundary data of Al Marj region and filtered the area of interest (AOI).\n// Import the administrative boundary of Libya and filter the target region (Al Marj) as the area of interest (AOI)\nvar admin = ee.FeatureCollection(\"projects/ee-hanmengyuan9/assets/libya\");\nvar geometry = admin.filter(ee.Filter.eq('shapeName', 'Al Marj'));\nMap.addLayer(geometry, {color: 'grey'}, 'Al Marj');\nAnd we selected the Sentinel-1 SAR image collection, set parameters and filter images for the flood periods.\n// Define the time periods before and after the flood event\nvar before_start = '2023-09-01';\nvar before_end = '2023-09-15';\nvar after_start = '2023-09-23';\nvar after_end = '2023-09-30';\n\n// Set sensor parameters for the satellite data collection\nvar polarization = (\"VH\", \"VV\");\nvar pass_direction = \"DESCENDING\";\nvar difference_threshold = 1.00;\n\n// Defining the area of interest based on the filtered geometry\nvar aoi = geometry;\n\n// Filtering the satellite image collection based on the specified parameters\nvar collection = ee.ImageCollection('COPERNICUS/S1_GRD')\n  .filter(ee.Filter.eq('instrumentMode', 'IW'))\n  .filter(ee.Filter.listContains('transmitterReceiverPolarisation', polarization))\n  .filter(ee.Filter.eq('orbitProperties_pass', pass_direction))\n  .filter(ee.Filter.eq('resolution_meters', 10))\n  .filterBounds(aoi)\n  .select(polarization);\n  \n// Filter the image collection to obtain images from before and after the flood\nvar before_collection = collection.filterDate(before_start, before_end);\nvar after_collection = collection.filterDate(after_start, after_end);\nThen, we mosaiced, clipped, and applied speckle noise reduction to the filtered before and after image collections to optimize data quality.\n// Create a mosaic of selected tiles and clip to study area\nvar before = before_collection.mosaic().clip(aoi);\nvar after = after_collection.mosaic().clip(aoi);\n\n// Apply reduce the radar speckle by smoothing  \nvar smoothing_radius = 50;\nvar before_filtered = before.focal_mean(smoothing_radius, 'circle', 'meters');\nvar after_filtered = after.focal_mean(smoothing_radius, 'circle', 'meters');\n1.2 Flood Extent Extraction\nWe extracted the flood extent using threshold segmentation and further optimize the flood extent using surface water data, connectivity analysis, and terrain filtering.\n// Calculate the difference between the before and after images\nvar difference = after_filtered.divide(before_filtered);\n\n// Apply the predefined difference-threshold and create the flood extent mask \nvar threshold = difference_threshold;\nvar difference_binary = difference.gt(threshold);\n\n// Refine flood result using additional datasets\n      \n      // Include JRC layer on surface water seasonality to mask flood pixels from areas\n      // of \"permanent\" water (where there is water &gt; 10 months of the year)\n      var swater = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('seasonality');\n      var swater_mask = swater.gte(10).updateMask(swater.gte(10));\n      \n      // Flooded layer where perennial water bodies (water &gt; 10 mo/yr) is assigned a 0 value\n      var flooded_mask = difference_binary.where(swater_mask,0);\n      // final flooded area without pixels in perennial waterbodies\n      var flooded = flooded_mask.updateMask(flooded_mask);\n      \n      // Compute connectivity of pixels to eliminate those connected to 8 or fewer neighbours\n      // This operation reduces noise of the flood extent product \n      var connections = flooded.connectedPixelCount();    \n      var flooded = flooded.updateMask(connections.gte(8));\n      \n      // Mask out areas with more than 5 percent slope using a Digital Elevation Model \n      var DEM = ee.Image('WWF/HydroSHEDS/03VFDEM');\n      var terrain = ee.Algorithms.Terrain(DEM);\n      var slope = terrain.select('slope');\n      var flooded = flooded.updateMask(slope.lt(5));\n1.3 Flood Area Calculation\nWe calculated the flood inundation area based on pixel area and regional statistics.\n// Calculate flood extent area\n// Create a raster layer containing the area information of each pixel \nvar flood_pixelarea = flooded.select(polarization)\n  .multiply(ee.Image.pixelArea());\n\n// Sum the areas of flooded pixels\n// default is set to 'bestEffort: true' in order to reduce computation time, for a more \n// accurate result set bestEffort to false and increase 'maxPixels'. \nvar flood_stats = flood_pixelarea.reduceRegion({\n  reducer: ee.Reducer.sum(),              \n  geometry: aoi,\n  scale: 10, // native resolution \n  //maxPixels: 1e9,\n  bestEffort: true\n  });\n\n// Convert the flood extent to hectares (area calculations are originally given in meters)  \nvar flood_area_ha = flood_stats\n  .getNumber(polarization)\n  .divide(10000)\n  .round();\n2. Identify buildings affected by flooding\n2.1 Data preprocessing\nThe obtained Sentinel-2 image is filtered, processed, cropped, and a preliminary image is generated. A mask is then applied to the data based on NDWI and NDVI thresholds (0.3 and 0.2, respectively) to filter out non-water and low vegetation cover areas.\n// Define an array of band names for Sentinel-2 imagery\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'];\n\n// Create an ImageCollection of Sentinel-2 surface reflectance images\nvar sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR')\n  .filter(ee.Filter.date(before_start, after_end)) // Filter images by date range\n  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) // Filter images with less than 10% cloud cover\n  .mean() // Compute the mean value of each pixel across the filtered images\n  .select(bands) // Select the specified bands\n  .clip(geometry); // Clip the resulting image to the specified geometry\n\n// Define visualization parameters for RGB composite\nvar s_rgb = {\n  min: 0.0, // Minimum value for stretching the image\n  max: 3000, // Maximum value for stretching the image\n  bands: ['B4', 'B3', 'B2'], // Red, Green, Blue bands for the composite\n  opacity: 1 // Opacity of the layer\n};\n\n// Calculate NDVI (Normalized Difference Vegetation Index)\nvar ndvi = sentinel2.normalizedDifference(['B8', 'B4']).select(['nd'], ['ndvi']);\n\n// Calculate NDWI (Normalized Difference Water Index)\nvar ndwi = sentinel2.normalizedDifference(['B3', 'B8']).select(['nd'], ['ndwi']);\n\n// Create a masked image based on NDWI and NDVI thresholds and add NDVI band\nvar image = sentinel2\n  .updateMask(ndwi.lt(0.3)) // Mask pixels with NDWI less than 0.3\n  .updateMask(ndvi.lt(0.2)) // Mask pixels with NDVI less than 0.2\n  .addBands(ndvi); // Add the NDVI band to the image\n2.2 RF analysis\nTo improve the accuracy of the training building model, we will classify and identify different features (such as buildings, farmland, deserts, etc.) together by Random Forest.\nvar building_points = ee.FeatureCollection.randomPoints(buildings2, 3000).map(function(i) {\n  return i.set({'class': 0});\n});\nvar farm_points = ee.FeatureCollection.randomPoints(farmland, 3000).map(function(i) {\n  return i.set({'class': 1});\n});\nvar desert_points = ee.FeatureCollection.randomPoints(desert, 3000).map(function(i) {\n  return i.set({'class': 2});\n});\nvar water_points = ee.FeatureCollection.randomPoints(water, 3000).map(function(i) {\n  return i.set({'class': 3});\n});\nvar road_points = ee.FeatureCollection.randomPoints(road, 3000).map(function(i) {\n  return i.set({'class': 4});\n});\nvar parking_points = ee.FeatureCollection.randomPoints(parking, 3000).map(function(i) {\n  return i.set({'class': 5});\n});\nCombine the random points and split them into training and validation sets and we used the split ratio equal to 0.7.\n// Combine the random points for each land cover class into a single FeatureCollection\nvar sample = ee.FeatureCollection([\n  building_points,\n  farm_points,\n  desert_points,\n  water_points,\n  road_points,\n  parking_points\n])\n.flatten()  // Flatten the FeatureCollection\n.randomColumn(); // Add a random column to the FeatureCollection for splitting\n\n// Define the split ratio for training and validation samples\nvar split = 0.7;\n\n// Create training sample by filtering the sample FeatureCollection where the random column is less than the split ratio\nvar training_sample = sample.filter(ee.Filter.lt('random', split));\n\n// Create validation sample by filtering the sample FeatureCollection where the random column is greater than or equal to the split ratio\nvar validation_sample = sample.filter(ee.Filter.gte('random', split));\nSample the image using the training and validation points and Train a Random Forest classifier using the training samples.\n// Sample the image using the training sample\nvar training = image.sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\n// Sample the image using the validation sample\nvar validation = image.sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Train a Random Forest classifier using the training samples\nvar model = ee.Classifier.smileRandomForest(400)\n  .train(training, 'class');\nClassify the image using the trained model and extract the building class from the prediction.\n// Classify the image using the trained model\nvar prediction = image.classify(model);\n\n// Extract the building class from the prediction by masking pixels that are not classified as buildings (class 0)\nvar building_prediction = prediction.updateMask(prediction.eq(0));\n2.3 Calculate the built-up areas, croplands, buildings footprint and population after the disaster\nImport ESA WorldCover into the map and extract cropland and built-up land from it. By intersecting the Build-up land with the identified buildings, we will focus on the disaster situation of the buildings in the built-up area. Finally, the flood data mask was used to obtain the cropland, built-up areas, building footprint and population after the disaster.\n// Use the ESA WorldCover 10m v200 dataset\nvar dataset = ee.ImageCollection('ESA/WorldCover/v200')\n  .first() // Take the first Image\n  .select('Map') // Select the 'Map' band\n  .clip(aoi);\n\n// Get the projection information of ESA WorldCover\nvar worldCoverProjection = dataset.projection();\n\n// Reproject the flood layer to the scale of ESA WorldCover\nvar flooded_res = flooded.reproject({\n  crs: worldCoverProjection\n});\n\n// Add the ESA WorldCover layer to the map\nvar worldCoverVis = {\n  min: 10,\n  max: 100,\n  palette: [\n    '#006400', '#ffbb22', '#ffff4c', '#f096ff', '#fa0000',\n    '#b4b4b4', '#f0f0f0', '#0064c8', '#0096a0', '#00cf75',\n    '#fae6a0'\n  ]\n};\nMap.addLayer(dataset, worldCoverVis, 'ESA WorldCover');\n\n// Load the dataset\nvar worldCover = ee.ImageCollection('ESA/WorldCover/v200').first().clip(geometry);\nvar builtUp = worldCover.updateMask(worldCover.eq(50));\nvar builtUpVisParams = {\n  palette: ['ff0000'],\n  opacity: 0.7\n};\n\n// Add built-up area layer to the map\nMap.addLayer(builtUp, builtUpVisParams, 'Built-up Area');\n\n// Select cropland category (class code 40 corresponds to Rain-fed cropland)\nvar cropland = dataset.eq(40);\n\n// Calculate affected cropland area\nvar croplandAffected = flooded_res.updateMask(cropland);\n\n// Select built-up category (class code 50)\nvar buildland = dataset.eq(50);\nvar class_prediction = prediction.updateMask(prediction.eq(0));\n\n// Mask the classification result using the built-up area image\nvar maskedClassPrediction = class_prediction.updateMask(builtUp);\n\n// Add the masked classification result to the map\nMap.addLayer(maskedClassPrediction, {palette:'red'}, 'Built-up Area Prediction');\n\n// ---------- Calculate flood-affected population -----------\n\n// Load population data from GHSL\nvar population = ee.Image('JRC/GHSL/P2023A/GHS_POP/2020')\n  .clip(aoi);\n\n// Get GHSL projection\nvar GHSLprojection = population.projection();\n\n// Reproject flood layer to GHSL scale\nvar flooded_res1 = flooded.reproject({\n  crs: GHSLprojection\n});\n\n// Calculate flood-affected population by applying the flood and population masks\nvar floodedPopulation = population\n  .updateMask(flooded_res1)\n  .updateMask(population);\nCalculate the area of affected croplands, built-up areas, building footprint, and population.\n// ------------------ Cropland ------------\n// Calculate the area of each affected cropland pixel\nvar affectedCroplandArea = Affected_Cropland.multiply(ee.Image.pixelArea());\n\n// Sum the area of affected cropland\nvar croplandStats = affectedCroplandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n}); \n\nvar croplandAreaHectares = ee.Number(croplandStats.get('b1')).divide(10000).round();\n// Calculate the area of each affected pixel in the built-up area\nvar affectedBuildlandArea = Affected_Builtup.multiply(ee.Image.pixelArea());\n\n// -------------builtup area ----------------\n\n// Sum the area of the affected built-up area\nvar buildlandStats = affectedBuildlandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n});\n\nvar buildlandAreaHectares = ee.Number(buildlandStats.get('b1')).divide(10000).round();\n\n// ----------   population ----------- \n\nvar populationStats = Flooded_Population.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 100, \n  maxPixels: 1e9\n  //bestEffort: true\n});\n\nvar affectedPopulation = ee.Number(populationStats.get('b1')).round();\n\n// ----------- buildings footprint -------------\n\nvar buildingFootprintArea = buildingsFootprint.multiply(ee.Image.pixelArea());\nvar buildingFootprintAreaSum = buildingFootprintArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, \n  maxPixels: 1e13\n});\n\nvar buildingAreaHectares = ee.Number(buildingFootprintAreaSum.get('b1')).divide(10000).round();\n\n\n//  -------------- output ------------\nprint(croplandAreaHectares);\nprint(buildlandAreaHectares);\nprint(affectedPopulation);\nprint(buildingAreaHectares);\n\n\n\n1.Create a map instance and Set the center point and zoom level of the map\nvar map = ui.Map();\nmap.centerObject(aoi,10.5);\n\nThe results of the data volt analysis are displayed in the lower right corner of the user interface. This user interface features a panel with a series of informational sections and a legend, organized as follows:\n\nResults Section: At the top of the panel, there is a “Results” header indicating the category of information provided. Below the header, several pieces of information are listed: - Flood Status: Dates between which the flood status is reported (2023-09-23 to 2023-09-30). - Estimated Flood Extent: The area affected by the flood, as estimated from Sentinel-1 imagery, is listed as 1023443 hectares. - Damaged Built-Up Areas: An estimate of the damaged urban area is provided (2943 hectares). - Damaged Crop Land Areas: The extent of the damage to cropland is reported (142837 hectares). - Estimated Damaged Buildings Land Area: This section lists the estimated area of damaged buildings (367 hectares). - Estimated Affected Population: The number of people estimated to be affected by the event (200094).\nLegend: Below the results and separated by a horizontal line, there is a legend that visually correlates colors with types of areas or statuses: - A blue square represents areas affected by the flood. - A red square denotes built-up areas. - A purple square indicates cropland. - A darker purple square is used for damaged buildings.\nEach legend item consists of a colored square followed by a text label that describes what the color represents in the context of the map being displayed.\n// Create a vertical panel that acts as a container for the entire right panel\nvar rightPanel = ui.Panel({\n  style: {\n    position: 'bottom-right',\n    padding: '8px 15px',\n    width: '250px' // Adjust the width as needed\n  },\n  layout: ui.Panel.Layout.flow('vertical') // Set the panel vertical layout\n});\n\n// Create a panel to display the results\nvar results = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0 0 8px 0' // Add a bottom margin to separate it from the legend panel\n  }\n});\n\n// Prepare the visualization parameters of the labels\n// Defines visual style parameters for text labels\nvar textVis = {\n  'margin':'0px 8px 2px 0px',\n  'fontWeight':'bold'\n};\nvar numberVIS = {\n  'margin':'0px 0px 15px 0px', \n  'color':'bf0f19',\n  'fontWeight':'bold'\n};\nvar subTextVis = {\n  'margin':'0px 0px 2px 0px',\n  'fontSize':'12px',\n  'color':'grey'\n};\n\nvar titleTextVis = {\n  'margin':'0px 0px 15px 0px',\n  'fontSize': '18px', \n  'color': '3333ff'\n};\n\n// Create text labels for titles and data\nvar title = ui.Label('Results', titleTextVis);\nvar text1 = ui.Label('Flood status between:', textVis);\nvar number1 = ui.Label(after_start.concat(\" and \",after_end), numberVIS);\n\n// The default text label is \"Please wait...\" Then replace it with the actual data\nvar text2 = ui.Label('Estimated flood extent:', textVis);\nvar text2_2 = ui.Label('Please wait...', subTextVis);\ndates(after_collection).evaluate(function(val){text2_2.setValue('based on Sentinel-1 imagery '+val)});\nvar number2 = ui.Label('Please wait...', numberVIS); \nflood_area_ha.evaluate(function(val){number2.setValue(val+' hectares')});\n\n//some more data\nvar text3 = ui.Label('Damaged Built-Up Areas (Ha):', textVis);\nvar number3 = ui.Label('Please wait...', numberVIS);\ndamagedBuildUpAreas.evaluate(function(val) {\n  number3.setValue(val + ' hectares');\n});\n\nvar text4 = ui.Label('Damaged Crop Land Areas (Ha):', textVis);\nvar number4 = ui.Label('Please wait...', numberVIS);\ndamagedCropLandAreas.evaluate(function(val) {\n  number4.setValue(val + ' hectares');\n});\n\nvar text5 = ui.Label('Estimated Damaged Buildings Land Area (Ha):', textVis);\nvar number5 = ui.Label('Please wait...', numberVIS);\nestimatedDamagedBuildings.evaluate(function(val) {\n  number5.setValue(val + ' hectares');\n});\n\nvar text6 = ui.Label('Estimated Affected Population:', textVis);\nvar number6 = ui.Label('Please wait...', numberVIS);\nestimatedAffectedPopulation.evaluate(function(val) {\n  number6.setValue(val.toString());\n});\n\nresults.add(ui.Panel([\n        title,\n        text1,\n        number1,\n        text2,\n        text2_2,\n        number2,\n        text3,\n        number3,\n        text4,\n        number4,\n        text5,\n        number5,\n        text6,\n        number6,\n       ]\n      ));\n      \n      \n// Create a function to generate legend items with colors and labels\nfunction createLegendItem(color, label) {\n  return ui.Panel({\n    widgets: [\n      ui.Label('', {\n        backgroundColor: color,\n        padding: '8px', // Adjust to fit the size of the legend color block\n        margin: '0 4px 0 0',\n      }),\n      ui.Label(label, {\n        margin: '-8px 0 0 4px',\n        padding: '8px 0px', // Adjust the upper and lower margins of the text to align it vertically\n        fontSize: '12px'\n      })\n    ],\n    layout: ui.Panel.Layout.Flow('horizontal')\n  });\n}\n\n\n// Create a legend panel and add a title\nvar legend = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0'\n  }\n});\nlegend.add(ui.Label('Legend', {fontWeight: 'bold', fontSize: '16px', margin: '0 0 4px 0'}));\n\n// Create a legend item using a function and add it to the legend panel\nlegend.add(createLegendItem('blue', 'Flood'));\nlegend.add(createLegendItem('#fa0000', 'Built-up'));\nlegend.add(createLegendItem('#f096ff', 'CropLand'));\nlegend.add(createLegendItem('purple', 'Damaged Building'));\n\n// Add a splitter between the results panel and the Legend panel\nvar separatorLine = ui.Panel({\n  style: {\n    height: '2px',\n    backgroundColor: 'black',\n    margin: '8px 0'\n  }\n});\n\n// Add the results panel and Legend panel to the right panel container\nrightPanel.add(results);\nrightPanel.add(separatorLine); \nrightPanel.add(legend);\n\n// Adds the entire right panel to the map interface\nMap.add(rightPanel);"
  },
  {
    "objectID": "CASA0025_Project_Template/index.html#link-to-this-project",
    "href": "CASA0025_Project_Template/index.html#link-to-this-project",
    "title": "CASA00025 Group Project Title Here",
    "section": "",
    "text": "https://github.com/MengyuanHan1/BigData/tree/main"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA00025 Group Project: Al Marj Flood Impact Comprehensive Evaluation System",
    "section": "",
    "text": "This project crafts an application within Google Earth Engine to evaluate the repercussions of the substantial flood event that transpired in Al Marj, Libya, during September 2023. The application integrates satellite imagery for flood mapping, land cover data for analyzing affected land cover types, and random forest classification to detect affected buildings. Additionally, it utilizes population grid data to estimate the number of people affected by the flood. The results provide a comprehensive assessment of the flood extent, affected land cover areas, impacted buildings, and the affected population, which is crucial for informing disaster response and management efforts.\n!(Study Area)[images/Study Area.png]\n\n\nIn September 2023, severe flooding in Al Marj, Libya, caused significant damage to infrastructure and displaced numerous people. Rapid and accurate assessment of the flood’s impact is essential for effective disaster response and management. This application aims to address the following key research questions:\n\nWhat is the total area affected by the flooding?\nWhat is the extent of damage affected by the flooding to different land cover types, particularly agricultural land and built-up areas? How the the flooding affact the area of Buildings?\nHow many people are estimated to be affected by the flooding?\n\n\n\n\nGovernment departments and international organizations:\nBy visualizing flooded areas, affected buildings, and population data, our project enables these departments to quickly and accurately assess the disaster situation. This facilitates effective rescue and reconstruction planning, as well as optimization of future urban development, land use, and drainage systems.\nAffected communities and the public:\nThe spatial distribution map of the flooded area generated by the project can help the affected communities and the public intuitively understand the scope and severity of the flood impact, and take timely countermeasures.\n\n\n\nSentinel-1 Satellite Image: Sentinel-1 collects C-band synthetic aperture radar (SAR) imagery at a variety of polarizations and resolutions. We use it to detect flood coverage.\nSentinel-2 Satellite Image: Sentinel-2 is a wide-swath, high-resolution, multispectral imaging mission with a global 5-day revisit frequency.\nSurface Water:JRC Global Surface Water Metadata contains maps of the location and temporal distribution of surface water. Year-round water is filtered using the JRC Global Water Seasonal Data layer.\nDEM:WWF HydroSHEDS Void-Filled DEM excludes areas with gradients greater than 5%.\nLand Classification: ESA WorldCover 10m v200 ESA WorldCover10m contains 11 land cover classes based on Sentinel-1 and Sentinel-2 data.\nPopulation: GHSL: Global population surfaces 1975-2030 contains the spatial distribution of residential population, expressed as the absolute number of inhabitants of the cell.\n\n\n\nFlood extent mapping: \nThe areas affected by the flood are identified by calculating the difference between pre-flood and post-flood images.\nLand cover impact assessment: By overlaying the flood extent layer with the land cover, the affected built-up areas and croplands are assessed.\nBuilding damage assessment: \nThe random forest classification algorithm is applied to the Sentinel-2 multispectral imagery to extract buildings within the built-up areas. The accuracy is evaluated using a confusion matrix.\nPopulation impact assessment: \nBy overlaying the flood extent map with the population grid, the application calculates the total number of individuals residing within the inundated areas.\n ### Interface\nUsers will be able to interact with the perform:\n\nDraw an Area of Interest (AOI): Users can draw a polygon on the map to define a specific area they want to analyze.\nObtain information: The application will process the data within the AOI and display the key indicators in the results panel, including Total Area, Flood Extent Areas, Damaged Built-Up Areas, Damaged Cropland Areas, Estimated Affected Population.\nVisualization: The map will display visual overlays within the AOI that include Affected Built-Up, Affected Cropland, A ffected Building Footprints, and Flood Areas. This setup clearly identifies flood-impacted regions.\n\n\n\n\n\nThis is our Earth Engine application.\n\n\n\n\n\n\n\n\n\n1. Flood Area Analysis\n1.1 Data Preprocessing\nWe imported the administrative boundary data of Al Marj region and filtered the area of interest (AOI).\n// Import the administrative boundary of Libya and filter the target region (Al Marj) as the area of interest (AOI)\nvar admin = ee.FeatureCollection(\"projects/ee-hanmengyuan9/assets/libya\");\nvar geometry = admin.filter(ee.Filter.eq('shapeName', 'Al Marj'));\nMap.addLayer(geometry, {color: 'grey'}, 'Al Marj');\nAnd we selected the Sentinel-1 SAR image collection, set parameters and filter images for the flood periods.\n// Define the time periods before and after the flood event\nvar before_start = '2023-09-01';\nvar before_end = '2023-09-15';\nvar after_start = '2023-09-23';\nvar after_end = '2023-09-30';\n\n// Set sensor parameters for the satellite data collection\nvar polarization = (\"VH\", \"VV\");\nvar pass_direction = \"DESCENDING\";\nvar difference_threshold = 1.00;\n\n// Defining the area of interest based on the filtered geometry\nvar aoi = geometry;\n\n// Filtering the satellite image collection based on the specified parameters\nvar collection = ee.ImageCollection('COPERNICUS/S1_GRD')\n  .filter(ee.Filter.eq('instrumentMode', 'IW'))\n  .filter(ee.Filter.listContains('transmitterReceiverPolarisation', polarization))\n  .filter(ee.Filter.eq('orbitProperties_pass', pass_direction))\n  .filter(ee.Filter.eq('resolution_meters', 10))\n  .filterBounds(aoi)\n  .select(polarization);\n  \n// Filter the image collection to obtain images from before and after the flood\nvar before_collection = collection.filterDate(before_start, before_end);\nvar after_collection = collection.filterDate(after_start, after_end);\nThen, we mosaiced, clipped, and applied speckle noise reduction to the filtered before and after image collections to optimize data quality.\n// Create a mosaic of selected tiles and clip to study area\nvar before = before_collection.mosaic().clip(aoi);\nvar after = after_collection.mosaic().clip(aoi);\n\n// Apply reduce the radar speckle by smoothing  \nvar smoothing_radius = 50;\nvar before_filtered = before.focal_mean(smoothing_radius, 'circle', 'meters');\nvar after_filtered = after.focal_mean(smoothing_radius, 'circle', 'meters');\n1.2 Flood Extent Extraction\nWe extracted the flood extent using threshold segmentation and further optimize the flood extent using surface water data, connectivity analysis, and terrain filtering.\n// Calculate the difference between the before and after images\nvar difference = after_filtered.divide(before_filtered);\n\n// Apply the predefined difference-threshold and create the flood extent mask \nvar threshold = difference_threshold;\nvar difference_binary = difference.gt(threshold);\n\n// Refine flood result using additional datasets\n      \n      // Include JRC layer on surface water seasonality to mask flood pixels from areas\n      // of \"permanent\" water (where there is water &gt; 10 months of the year)\n      var swater = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('seasonality');\n      var swater_mask = swater.gte(10).updateMask(swater.gte(10));\n      \n      // Flooded layer where perennial water bodies (water &gt; 10 mo/yr) is assigned a 0 value\n      var flooded_mask = difference_binary.where(swater_mask,0);\n      // final flooded area without pixels in perennial waterbodies\n      var flooded = flooded_mask.updateMask(flooded_mask);\n      \n      // Compute connectivity of pixels to eliminate those connected to 8 or fewer neighbours\n      // This operation reduces noise of the flood extent product \n      var connections = flooded.connectedPixelCount();    \n      var flooded = flooded.updateMask(connections.gte(8));\n      \n      // Mask out areas with more than 5 percent slope using a Digital Elevation Model \n      var DEM = ee.Image('WWF/HydroSHEDS/03VFDEM');\n      var terrain = ee.Algorithms.Terrain(DEM);\n      var slope = terrain.select('slope');\n      var flooded = flooded.updateMask(slope.lt(5));\n1.3 Flood Area Calculation\nWe calculated the flood inundation area based on pixel area and regional statistics.\n// Calculate flood extent area\n// Create a raster layer containing the area information of each pixel \nvar flood_pixelarea = flooded.select(polarization)\n  .multiply(ee.Image.pixelArea());\n\n// Sum the areas of flooded pixels\n// default is set to 'bestEffort: true' in order to reduce computation time, for a more \n// accurate result set bestEffort to false and increase 'maxPixels'. \nvar flood_stats = flood_pixelarea.reduceRegion({\n  reducer: ee.Reducer.sum(),              \n  geometry: aoi,\n  scale: 10, // native resolution \n  //maxPixels: 1e9,\n  bestEffort: true\n  });\n\n// Convert the flood extent to hectares (area calculations are originally given in meters)  \nvar flood_area_ha = flood_stats\n  .getNumber(polarization)\n  .divide(10000)\n  .round();\n2. Identify buildings affected by flooding\n2.1 Data preprocessing\nThe obtained Sentinel-2 image is filtered, processed, cropped, and a preliminary image is generated. A mask is then applied to the data based on NDWI and NDVI thresholds (0.3 and 0.2, respectively) to filter out non-water and low vegetation cover areas.\n// Define an array of band names for Sentinel-2 imagery\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'];\n\n// Create an ImageCollection of Sentinel-2 surface reflectance images\nvar sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR')\n  .filter(ee.Filter.date(before_start, after_end)) // Filter images by date range\n  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) // Filter images with less than 10% cloud cover\n  .mean() // Compute the mean value of each pixel across the filtered images\n  .select(bands) // Select the specified bands\n  .clip(geometry); // Clip the resulting image to the specified geometry\n\n// Define visualization parameters for RGB composite\nvar s_rgb = {\n  min: 0.0, // Minimum value for stretching the image\n  max: 3000, // Maximum value for stretching the image\n  bands: ['B4', 'B3', 'B2'], // Red, Green, Blue bands for the composite\n  opacity: 1 // Opacity of the layer\n};\n\n// Calculate NDVI (Normalized Difference Vegetation Index)\nvar ndvi = sentinel2.normalizedDifference(['B8', 'B4']).select(['nd'], ['ndvi']);\n\n// Calculate NDWI (Normalized Difference Water Index)\nvar ndwi = sentinel2.normalizedDifference(['B3', 'B8']).select(['nd'], ['ndwi']);\n\n// Create a masked image based on NDWI and NDVI thresholds and add NDVI band\nvar image = sentinel2\n  .updateMask(ndwi.lt(0.3)) // Mask pixels with NDWI less than 0.3\n  .updateMask(ndvi.lt(0.2)) // Mask pixels with NDVI less than 0.2\n  .addBands(ndvi); // Add the NDVI band to the image\n2.2 RF analysis\nTo improve the accuracy of the training building model, we will classify and identify different features (such as buildings, farmland, deserts, etc.) together by Random Forest.\nvar building_points = ee.FeatureCollection.randomPoints(buildings2, 3000).map(function(i) {\n  return i.set({'class': 0});\n});\nvar farm_points = ee.FeatureCollection.randomPoints(farmland, 3000).map(function(i) {\n  return i.set({'class': 1});\n});\nvar desert_points = ee.FeatureCollection.randomPoints(desert, 3000).map(function(i) {\n  return i.set({'class': 2});\n});\nvar water_points = ee.FeatureCollection.randomPoints(water, 3000).map(function(i) {\n  return i.set({'class': 3});\n});\nvar road_points = ee.FeatureCollection.randomPoints(road, 3000).map(function(i) {\n  return i.set({'class': 4});\n});\nvar parking_points = ee.FeatureCollection.randomPoints(parking, 3000).map(function(i) {\n  return i.set({'class': 5});\n});\nCombine the random points and split them into training and validation sets and we used the split ratio equal to 0.7.\n// Combine the random points for each land cover class into a single FeatureCollection\nvar sample = ee.FeatureCollection([\n  building_points,\n  farm_points,\n  desert_points,\n  water_points,\n  road_points,\n  parking_points\n])\n.flatten()  // Flatten the FeatureCollection\n.randomColumn(); // Add a random column to the FeatureCollection for splitting\n\n// Define the split ratio for training and validation samples\nvar split = 0.7;\n\n// Create training sample by filtering the sample FeatureCollection where the random column is less than the split ratio\nvar training_sample = sample.filter(ee.Filter.lt('random', split));\n\n// Create validation sample by filtering the sample FeatureCollection where the random column is greater than or equal to the split ratio\nvar validation_sample = sample.filter(ee.Filter.gte('random', split));\nSample the image using the training and validation points and Train a Random Forest classifier using the training samples.\n// Sample the image using the training sample\nvar training = image.sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\n// Sample the image using the validation sample\nvar validation = image.sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Train a Random Forest classifier using the training samples\nvar model = ee.Classifier.smileRandomForest(400)\n  .train(training, 'class');\nClassify the image using the trained model and extract the building class from the prediction.\n// Classify the image using the trained model\nvar prediction = image.classify(model);\n\n// Extract the building class from the prediction by masking pixels that are not classified as buildings (class 0)\nvar building_prediction = prediction.updateMask(prediction.eq(0));\n3.Calculate the built-up areas, croplands, buildings footprint and population after the disaster\nImport ESA WorldCover into the map and extract cropland and built-up land from it. By intersecting the Build-up land with the identified buildings, we will focus on the disaster situation of the buildings in the built-up area. Finally, the flood data mask was used to obtain the cropland, built-up areas, building footprint and population after the disaster.\n// Use the ESA WorldCover 10m v200 dataset\nvar dataset = ee.ImageCollection('ESA/WorldCover/v200')\n  .first() // Take the first Image\n  .select('Map') // Select the 'Map' band\n  .clip(aoi);\n\n// Get the projection information of ESA WorldCover\nvar worldCoverProjection = dataset.projection();\n\n// Reproject the flood layer to the scale of ESA WorldCover\nvar flooded_res = flooded.reproject({\n  crs: worldCoverProjection\n});\n\n// Add the ESA WorldCover layer to the map\nvar worldCoverVis = {\n  min: 10,\n  max: 100,\n  palette: [\n    '#006400', '#ffbb22', '#ffff4c', '#f096ff', '#fa0000',\n    '#b4b4b4', '#f0f0f0', '#0064c8', '#0096a0', '#00cf75',\n    '#fae6a0'\n  ]\n};\nMap.addLayer(dataset, worldCoverVis, 'ESA WorldCover');\n\n// Select cropland category (class code 40 corresponds to Rain-fed cropland)\nvar cropland = dataset.eq(40);\n// Calculate affected cropland area\nvar croplandAffected = flooded_res.updateMask(cropland);\n\n// Select built-up category (class code 50)\nvar buildland = dataset.eq(50);\nvar class_prediction = prediction.updateMask(prediction.eq(0));\n\n// Mask the classification result using the built-up area image\nvar maskedClassPrediction = class_prediction.updateMask(builtUp);\n// Add the masked classification result to the map\nMap.addLayer(maskedClassPrediction, {palette:'red'}, 'Built-up Area Prediction');\n\n// ---------- Calculate flood-affected population -----------\n// Load population data from GHSL\nvar population = ee.Image('JRC/GHSL/P2023A/GHS_POP/2020')\n  .clip(aoi);\n// Get GHSL projection\nvar GHSLprojection = population.projection();\n// Reproject flood layer to GHSL scale\nvar flooded_res1 = flooded.reproject({\n  crs: GHSLprojection\n});\n\n// Calculate flood-affected population by applying the flood and population masks\nvar floodedPopulation = population\n  .updateMask(flooded_res1)\n  .updateMask(population);\nCalculate the area of affected croplands, built-up areas, building footprint, and population.\n// ------------------ Cropland ------------\n// Calculate the area of each affected cropland pixel\nvar affectedCroplandArea = Affected_Cropland.multiply(ee.Image.pixelArea());\n// Sum the area of affected cropland\nvar croplandStats = affectedCroplandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n}); \n\nvar croplandAreaHectares = ee.Number(croplandStats.get('b1')).divide(10000).round();\n// Calculate the area of each affected pixel in the built-up area\nvar affectedBuildlandArea = Affected_Builtup.multiply(ee.Image.pixelArea());\n\n// -------------builtup area ----------------\n// Sum the area of the affected built-up area\nvar buildlandStats = affectedBuildlandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n});\n\nvar buildlandAreaHectares = ee.Number(buildlandStats.get('b1')).divide(10000).round();\n\n// ----------   population ----------- \n\nvar populationStats = Flooded_Population.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 100, \n  maxPixels: 1e9\n  //bestEffort: true\n});\n\nvar affectedPopulation = ee.Number(populationStats.get('b1')).round();\n\n// ----------- buildings footprint -------------\nvar buildingFootprintArea = buildingsFootprint.multiply(ee.Image.pixelArea());\nvar buildingFootprintAreaSum = buildingFootprintArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, \n  maxPixels: 1e13\n});\n\nvar buildingAreaHectares = ee.Number(buildingFootprintAreaSum.get('b1')).divide(10000).round();\n\n\n//  -------------- output ------------\nprint(croplandAreaHectares);\nprint(buildlandAreaHectares);\nprint(affectedPopulation);\nprint(buildingAreaHectares);\n\n\n\n\nCreate a map instance and Set the center point and zoom level of the map\n\nvar map = ui.Map();\nmap.centerObject(aoi,10.5);\n\nDisplay results The results of the data volt analysis are displayed in the lower right corner of the user interface. This user interface features a panel with a series of informational sections and a legend, organized as follows:\n\nResults Section: At the top of the panel, there is a “Results” header indicating the category of information provided. Below the header, several pieces of information are listed: - Flood Status: Dates between which the flood status is reported (2023-09-23 to 2023-09-30). - Estimated Flood Extent: The area affected by the flood, as estimated from Sentinel-1 imagery, is listed as 1023443 hectares. - Damaged Built-Up Areas: An estimate of the damaged urban area is provided (2943 hectares). - Damaged Crop Land Areas: The extent of the damage to cropland is reported (142837 hectares). - Estimated Damaged Buildings Land Area: This section lists the estimated area of damaged buildings (367 hectares). - Estimated Affected Population: The number of people estimated to be affected by the event (200094).\nLegend: Below the results and separated by a horizontal line, there is a legend that visually correlates colors with types of areas or statuses: - A blue square represents areas affected by the flood. - A red square denotes built-up areas. - A purple square indicates cropland. - A darker purple square is used for damaged buildings.\nEach legend item consists of a colored square followed by a text label that describes what the color represents in the context of the map being displayed.\n// Create a vertical panel that acts as a container for the entire right panel\nvar rightPanel = ui.Panel({\n  style: {\n    position: 'bottom-right',\n    padding: '8px 15px',\n    width: '250px' // Adjust the width as needed\n  },\n  layout: ui.Panel.Layout.flow('vertical') // Set the panel vertical layout\n});\n\n// Create a panel to display the results\nvar results = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0 0 8px 0' // Add a bottom margin to separate it from the legend panel\n  }\n});\n\n// Prepare the visualization parameters of the labels\n// Defines visual style parameters for text labels\nvar textVis = {\n  'margin':'0px 8px 2px 0px',\n  'fontWeight':'bold'\n};\nvar numberVIS = {\n  'margin':'0px 0px 15px 0px', \n  'color':'bf0f19',\n  'fontWeight':'bold'\n};\nvar subTextVis = {\n  'margin':'0px 0px 2px 0px',\n  'fontSize':'12px',\n  'color':'grey'\n};\n\nvar titleTextVis = {\n  'margin':'0px 0px 15px 0px',\n  'fontSize': '18px', \n  'color': '3333ff'\n};\n\n// Create text labels for titles and data\nvar title = ui.Label('Results', titleTextVis);\nvar text1 = ui.Label('Flood status between:', textVis);\nvar number1 = ui.Label(after_start.concat(\" and \",after_end), numberVIS);\n\n// The default text label is \"Please wait...\" Then replace it with the actual data\nvar text2 = ui.Label('Estimated flood extent:', textVis);\nvar text2_2 = ui.Label('Please wait...', subTextVis);\ndates(after_collection).evaluate(function(val){text2_2.setValue('based on Sentinel-1 imagery '+val)});\nvar number2 = ui.Label('Please wait...', numberVIS); \nflood_area_ha.evaluate(function(val){number2.setValue(val+' hectares')});\n\n//some more data\nvar text3 = ui.Label('Damaged Built-Up Areas (Ha):', textVis);\nvar number3 = ui.Label('Please wait...', numberVIS);\ndamagedBuildUpAreas.evaluate(function(val) {\n  number3.setValue(val + ' hectares');\n});\n\nvar text4 = ui.Label('Damaged Crop Land Areas (Ha):', textVis);\nvar number4 = ui.Label('Please wait...', numberVIS);\ndamagedCropLandAreas.evaluate(function(val) {\n  number4.setValue(val + ' hectares');\n});\n\nvar text5 = ui.Label('Estimated Damaged Buildings Land Area (Ha):', textVis);\nvar number5 = ui.Label('Please wait...', numberVIS);\nestimatedDamagedBuildings.evaluate(function(val) {\n  number5.setValue(val + ' hectares');\n});\n\nvar text6 = ui.Label('Estimated Affected Population:', textVis);\nvar number6 = ui.Label('Please wait...', numberVIS);\nestimatedAffectedPopulation.evaluate(function(val) {\n  number6.setValue(val.toString());\n});\n\nresults.add(ui.Panel([\n        title,\n        text1,\n        number1,\n        text2,\n        text2_2,\n        number2,\n        text3,\n        number3,\n        text4,\n        number4,\n        text5,\n        number5,\n        text6,\n        number6,\n       ]\n      ));\n      \n      \n// Create a function to generate legend items with colors and labels\nfunction createLegendItem(color, label) {\n  return ui.Panel({\n    widgets: [\n      ui.Label('', {\n        backgroundColor: color,\n        padding: '8px', // Adjust to fit the size of the legend color block\n        margin: '0 4px 0 0',\n      }),\n      ui.Label(label, {\n        margin: '-8px 0 0 4px',\n        padding: '8px 0px', // Adjust the upper and lower margins of the text to align it vertically\n        fontSize: '12px'\n      })\n    ],\n    layout: ui.Panel.Layout.Flow('horizontal')\n  });\n}\n\n\n// Create a legend panel and add a title\nvar legend = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0'\n  }\n});\nlegend.add(ui.Label('Legend', {fontWeight: 'bold', fontSize: '16px', margin: '0 0 4px 0'}));\n\n// Create a legend item using a function and add it to the legend panel\nlegend.add(createLegendItem('blue', 'Flood'));\nlegend.add(createLegendItem('#fa0000', 'Built-up'));\nlegend.add(createLegendItem('#f096ff', 'CropLand'));\nlegend.add(createLegendItem('purple', 'Damaged Building'));\n\n// Add a splitter between the results panel and the Legend panel\nvar separatorLine = ui.Panel({\n  style: {\n    height: '2px',\n    backgroundColor: 'black',\n    margin: '8px 0'\n  }\n});\n\n// Add the results panel and Legend panel to the right panel container\nrightPanel.add(results);\nrightPanel.add(separatorLine); \nrightPanel.add(legend);\n\n// Adds the entire right panel to the map interface\nMap.add(rightPanel);\n\n\n\n\nhttps://github.com/MengyuanHan1/BigData/tree/main"
  },
  {
    "objectID": "index.html#project-summary",
    "href": "index.html#project-summary",
    "title": "CASA00025 Group Project: Al Marj Flood Impact Comprehensive Evaluation System",
    "section": "",
    "text": "This project crafts an application within Google Earth Engine to evaluate the repercussions of the substantial flood event that transpired in Al Marj, Libya, during September 2023. The application integrates satellite imagery for flood mapping, land cover data for analyzing affected land cover types, and random forest classification to detect affected buildings. Additionally, it utilizes population grid data to estimate the number of people affected by the flood. The results provide a comprehensive assessment of the flood extent, affected land cover areas, impacted buildings, and the affected population, which is crucial for informing disaster response and management efforts.\n!(Study Area)[images/Study Area.png]\n\n\nIn September 2023, severe flooding in Al Marj, Libya, caused significant damage to infrastructure and displaced numerous people. Rapid and accurate assessment of the flood’s impact is essential for effective disaster response and management. This application aims to address the following key research questions:\n\nWhat is the total area affected by the flooding?\nWhat is the extent of damage affected by the flooding to different land cover types, particularly agricultural land and built-up areas? How the the flooding affact the area of Buildings?\nHow many people are estimated to be affected by the flooding?\n\n\n\n\nGovernment departments and international organizations:\nBy visualizing flooded areas, affected buildings, and population data, our project enables these departments to quickly and accurately assess the disaster situation. This facilitates effective rescue and reconstruction planning, as well as optimization of future urban development, land use, and drainage systems.\nAffected communities and the public:\nThe spatial distribution map of the flooded area generated by the project can help the affected communities and the public intuitively understand the scope and severity of the flood impact, and take timely countermeasures.\n\n\n\nSentinel-1 Satellite Image: Sentinel-1 collects C-band synthetic aperture radar (SAR) imagery at a variety of polarizations and resolutions. We use it to detect flood coverage.\nSentinel-2 Satellite Image: Sentinel-2 is a wide-swath, high-resolution, multispectral imaging mission with a global 5-day revisit frequency.\nSurface Water:JRC Global Surface Water Metadata contains maps of the location and temporal distribution of surface water. Year-round water is filtered using the JRC Global Water Seasonal Data layer.\nDEM:WWF HydroSHEDS Void-Filled DEM excludes areas with gradients greater than 5%.\nLand Classification: ESA WorldCover 10m v200 ESA WorldCover10m contains 11 land cover classes based on Sentinel-1 and Sentinel-2 data.\nPopulation: GHSL: Global population surfaces 1975-2030 contains the spatial distribution of residential population, expressed as the absolute number of inhabitants of the cell.\n\n\n\nFlood extent mapping: \nThe areas affected by the flood are identified by calculating the difference between pre-flood and post-flood images.\nLand cover impact assessment: By overlaying the flood extent layer with the land cover, the affected built-up areas and croplands are assessed.\nBuilding damage assessment: \nThe random forest classification algorithm is applied to the Sentinel-2 multispectral imagery to extract buildings within the built-up areas. The accuracy is evaluated using a confusion matrix.\nPopulation impact assessment: \nBy overlaying the flood extent map with the population grid, the application calculates the total number of individuals residing within the inundated areas.\n ### Interface\nUsers will be able to interact with the perform:\n\nDraw an Area of Interest (AOI): Users can draw a polygon on the map to define a specific area they want to analyze.\nObtain information: The application will process the data within the AOI and display the key indicators in the results panel, including Total Area, Flood Extent Areas, Damaged Built-Up Areas, Damaged Cropland Areas, Estimated Affected Population.\nVisualization: The map will display visual overlays within the AOI that include Affected Built-Up, Affected Cropland, A ffected Building Footprints, and Flood Areas. This setup clearly identifies flood-impacted regions."
  },
  {
    "objectID": "index.html#the-application",
    "href": "index.html#the-application",
    "title": "CASA00025 Group Project: Al Marj Flood Impact Comprehensive Evaluation System",
    "section": "",
    "text": "This is our Earth Engine application."
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "CASA00025 Group Project: Al Marj Flood Impact Comprehensive Evaluation System",
    "section": "",
    "text": "1. Flood Area Analysis\n1.1 Data Preprocessing\nWe imported the administrative boundary data of Al Marj region and filtered the area of interest (AOI).\n// Import the administrative boundary of Libya and filter the target region (Al Marj) as the area of interest (AOI)\nvar admin = ee.FeatureCollection(\"projects/ee-hanmengyuan9/assets/libya\");\nvar geometry = admin.filter(ee.Filter.eq('shapeName', 'Al Marj'));\nMap.addLayer(geometry, {color: 'grey'}, 'Al Marj');\nAnd we selected the Sentinel-1 SAR image collection, set parameters and filter images for the flood periods.\n// Define the time periods before and after the flood event\nvar before_start = '2023-09-01';\nvar before_end = '2023-09-15';\nvar after_start = '2023-09-23';\nvar after_end = '2023-09-30';\n\n// Set sensor parameters for the satellite data collection\nvar polarization = (\"VH\", \"VV\");\nvar pass_direction = \"DESCENDING\";\nvar difference_threshold = 1.00;\n\n// Defining the area of interest based on the filtered geometry\nvar aoi = geometry;\n\n// Filtering the satellite image collection based on the specified parameters\nvar collection = ee.ImageCollection('COPERNICUS/S1_GRD')\n  .filter(ee.Filter.eq('instrumentMode', 'IW'))\n  .filter(ee.Filter.listContains('transmitterReceiverPolarisation', polarization))\n  .filter(ee.Filter.eq('orbitProperties_pass', pass_direction))\n  .filter(ee.Filter.eq('resolution_meters', 10))\n  .filterBounds(aoi)\n  .select(polarization);\n  \n// Filter the image collection to obtain images from before and after the flood\nvar before_collection = collection.filterDate(before_start, before_end);\nvar after_collection = collection.filterDate(after_start, after_end);\nThen, we mosaiced, clipped, and applied speckle noise reduction to the filtered before and after image collections to optimize data quality.\n// Create a mosaic of selected tiles and clip to study area\nvar before = before_collection.mosaic().clip(aoi);\nvar after = after_collection.mosaic().clip(aoi);\n\n// Apply reduce the radar speckle by smoothing  \nvar smoothing_radius = 50;\nvar before_filtered = before.focal_mean(smoothing_radius, 'circle', 'meters');\nvar after_filtered = after.focal_mean(smoothing_radius, 'circle', 'meters');\n1.2 Flood Extent Extraction\nWe extracted the flood extent using threshold segmentation and further optimize the flood extent using surface water data, connectivity analysis, and terrain filtering.\n// Calculate the difference between the before and after images\nvar difference = after_filtered.divide(before_filtered);\n\n// Apply the predefined difference-threshold and create the flood extent mask \nvar threshold = difference_threshold;\nvar difference_binary = difference.gt(threshold);\n\n// Refine flood result using additional datasets\n      \n      // Include JRC layer on surface water seasonality to mask flood pixels from areas\n      // of \"permanent\" water (where there is water &gt; 10 months of the year)\n      var swater = ee.Image('JRC/GSW1_0/GlobalSurfaceWater').select('seasonality');\n      var swater_mask = swater.gte(10).updateMask(swater.gte(10));\n      \n      // Flooded layer where perennial water bodies (water &gt; 10 mo/yr) is assigned a 0 value\n      var flooded_mask = difference_binary.where(swater_mask,0);\n      // final flooded area without pixels in perennial waterbodies\n      var flooded = flooded_mask.updateMask(flooded_mask);\n      \n      // Compute connectivity of pixels to eliminate those connected to 8 or fewer neighbours\n      // This operation reduces noise of the flood extent product \n      var connections = flooded.connectedPixelCount();    \n      var flooded = flooded.updateMask(connections.gte(8));\n      \n      // Mask out areas with more than 5 percent slope using a Digital Elevation Model \n      var DEM = ee.Image('WWF/HydroSHEDS/03VFDEM');\n      var terrain = ee.Algorithms.Terrain(DEM);\n      var slope = terrain.select('slope');\n      var flooded = flooded.updateMask(slope.lt(5));\n1.3 Flood Area Calculation\nWe calculated the flood inundation area based on pixel area and regional statistics.\n// Calculate flood extent area\n// Create a raster layer containing the area information of each pixel \nvar flood_pixelarea = flooded.select(polarization)\n  .multiply(ee.Image.pixelArea());\n\n// Sum the areas of flooded pixels\n// default is set to 'bestEffort: true' in order to reduce computation time, for a more \n// accurate result set bestEffort to false and increase 'maxPixels'. \nvar flood_stats = flood_pixelarea.reduceRegion({\n  reducer: ee.Reducer.sum(),              \n  geometry: aoi,\n  scale: 10, // native resolution \n  //maxPixels: 1e9,\n  bestEffort: true\n  });\n\n// Convert the flood extent to hectares (area calculations are originally given in meters)  \nvar flood_area_ha = flood_stats\n  .getNumber(polarization)\n  .divide(10000)\n  .round();\n2. Identify buildings affected by flooding\n2.1 Data preprocessing\nThe obtained Sentinel-2 image is filtered, processed, cropped, and a preliminary image is generated. A mask is then applied to the data based on NDWI and NDVI thresholds (0.3 and 0.2, respectively) to filter out non-water and low vegetation cover areas.\n// Define an array of band names for Sentinel-2 imagery\nvar bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12'];\n\n// Create an ImageCollection of Sentinel-2 surface reflectance images\nvar sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR')\n  .filter(ee.Filter.date(before_start, after_end)) // Filter images by date range\n  .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)) // Filter images with less than 10% cloud cover\n  .mean() // Compute the mean value of each pixel across the filtered images\n  .select(bands) // Select the specified bands\n  .clip(geometry); // Clip the resulting image to the specified geometry\n\n// Define visualization parameters for RGB composite\nvar s_rgb = {\n  min: 0.0, // Minimum value for stretching the image\n  max: 3000, // Maximum value for stretching the image\n  bands: ['B4', 'B3', 'B2'], // Red, Green, Blue bands for the composite\n  opacity: 1 // Opacity of the layer\n};\n\n// Calculate NDVI (Normalized Difference Vegetation Index)\nvar ndvi = sentinel2.normalizedDifference(['B8', 'B4']).select(['nd'], ['ndvi']);\n\n// Calculate NDWI (Normalized Difference Water Index)\nvar ndwi = sentinel2.normalizedDifference(['B3', 'B8']).select(['nd'], ['ndwi']);\n\n// Create a masked image based on NDWI and NDVI thresholds and add NDVI band\nvar image = sentinel2\n  .updateMask(ndwi.lt(0.3)) // Mask pixels with NDWI less than 0.3\n  .updateMask(ndvi.lt(0.2)) // Mask pixels with NDVI less than 0.2\n  .addBands(ndvi); // Add the NDVI band to the image\n2.2 RF analysis\nTo improve the accuracy of the training building model, we will classify and identify different features (such as buildings, farmland, deserts, etc.) together by Random Forest.\nvar building_points = ee.FeatureCollection.randomPoints(buildings2, 3000).map(function(i) {\n  return i.set({'class': 0});\n});\nvar farm_points = ee.FeatureCollection.randomPoints(farmland, 3000).map(function(i) {\n  return i.set({'class': 1});\n});\nvar desert_points = ee.FeatureCollection.randomPoints(desert, 3000).map(function(i) {\n  return i.set({'class': 2});\n});\nvar water_points = ee.FeatureCollection.randomPoints(water, 3000).map(function(i) {\n  return i.set({'class': 3});\n});\nvar road_points = ee.FeatureCollection.randomPoints(road, 3000).map(function(i) {\n  return i.set({'class': 4});\n});\nvar parking_points = ee.FeatureCollection.randomPoints(parking, 3000).map(function(i) {\n  return i.set({'class': 5});\n});\nCombine the random points and split them into training and validation sets and we used the split ratio equal to 0.7.\n// Combine the random points for each land cover class into a single FeatureCollection\nvar sample = ee.FeatureCollection([\n  building_points,\n  farm_points,\n  desert_points,\n  water_points,\n  road_points,\n  parking_points\n])\n.flatten()  // Flatten the FeatureCollection\n.randomColumn(); // Add a random column to the FeatureCollection for splitting\n\n// Define the split ratio for training and validation samples\nvar split = 0.7;\n\n// Create training sample by filtering the sample FeatureCollection where the random column is less than the split ratio\nvar training_sample = sample.filter(ee.Filter.lt('random', split));\n\n// Create validation sample by filtering the sample FeatureCollection where the random column is greater than or equal to the split ratio\nvar validation_sample = sample.filter(ee.Filter.gte('random', split));\nSample the image using the training and validation points and Train a Random Forest classifier using the training samples.\n// Sample the image using the training sample\nvar training = image.sampleRegions({\n  collection: training_sample,\n  properties: ['class'],\n  scale: 10,\n});\n\n// Sample the image using the validation sample\nvar validation = image.sampleRegions({\n  collection: validation_sample,\n  properties: ['class'],\n  scale: 10\n});\n\n// Train a Random Forest classifier using the training samples\nvar model = ee.Classifier.smileRandomForest(400)\n  .train(training, 'class');\nClassify the image using the trained model and extract the building class from the prediction.\n// Classify the image using the trained model\nvar prediction = image.classify(model);\n\n// Extract the building class from the prediction by masking pixels that are not classified as buildings (class 0)\nvar building_prediction = prediction.updateMask(prediction.eq(0));\n3.Calculate the built-up areas, croplands, buildings footprint and population after the disaster\nImport ESA WorldCover into the map and extract cropland and built-up land from it. By intersecting the Build-up land with the identified buildings, we will focus on the disaster situation of the buildings in the built-up area. Finally, the flood data mask was used to obtain the cropland, built-up areas, building footprint and population after the disaster.\n// Use the ESA WorldCover 10m v200 dataset\nvar dataset = ee.ImageCollection('ESA/WorldCover/v200')\n  .first() // Take the first Image\n  .select('Map') // Select the 'Map' band\n  .clip(aoi);\n\n// Get the projection information of ESA WorldCover\nvar worldCoverProjection = dataset.projection();\n\n// Reproject the flood layer to the scale of ESA WorldCover\nvar flooded_res = flooded.reproject({\n  crs: worldCoverProjection\n});\n\n// Add the ESA WorldCover layer to the map\nvar worldCoverVis = {\n  min: 10,\n  max: 100,\n  palette: [\n    '#006400', '#ffbb22', '#ffff4c', '#f096ff', '#fa0000',\n    '#b4b4b4', '#f0f0f0', '#0064c8', '#0096a0', '#00cf75',\n    '#fae6a0'\n  ]\n};\nMap.addLayer(dataset, worldCoverVis, 'ESA WorldCover');\n\n// Select cropland category (class code 40 corresponds to Rain-fed cropland)\nvar cropland = dataset.eq(40);\n// Calculate affected cropland area\nvar croplandAffected = flooded_res.updateMask(cropland);\n\n// Select built-up category (class code 50)\nvar buildland = dataset.eq(50);\nvar class_prediction = prediction.updateMask(prediction.eq(0));\n\n// Mask the classification result using the built-up area image\nvar maskedClassPrediction = class_prediction.updateMask(builtUp);\n// Add the masked classification result to the map\nMap.addLayer(maskedClassPrediction, {palette:'red'}, 'Built-up Area Prediction');\n\n// ---------- Calculate flood-affected population -----------\n// Load population data from GHSL\nvar population = ee.Image('JRC/GHSL/P2023A/GHS_POP/2020')\n  .clip(aoi);\n// Get GHSL projection\nvar GHSLprojection = population.projection();\n// Reproject flood layer to GHSL scale\nvar flooded_res1 = flooded.reproject({\n  crs: GHSLprojection\n});\n\n// Calculate flood-affected population by applying the flood and population masks\nvar floodedPopulation = population\n  .updateMask(flooded_res1)\n  .updateMask(population);\nCalculate the area of affected croplands, built-up areas, building footprint, and population.\n// ------------------ Cropland ------------\n// Calculate the area of each affected cropland pixel\nvar affectedCroplandArea = Affected_Cropland.multiply(ee.Image.pixelArea());\n// Sum the area of affected cropland\nvar croplandStats = affectedCroplandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n}); \n\nvar croplandAreaHectares = ee.Number(croplandStats.get('b1')).divide(10000).round();\n// Calculate the area of each affected pixel in the built-up area\nvar affectedBuildlandArea = Affected_Builtup.multiply(ee.Image.pixelArea());\n\n// -------------builtup area ----------------\n// Sum the area of the affected built-up area\nvar buildlandStats = affectedBuildlandArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, // Use the original resolution of 10m from the WorldCover dataset\n  maxPixels: 1e9\n});\n\nvar buildlandAreaHectares = ee.Number(buildlandStats.get('b1')).divide(10000).round();\n\n// ----------   population ----------- \n\nvar populationStats = Flooded_Population.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 100, \n  maxPixels: 1e9\n  //bestEffort: true\n});\n\nvar affectedPopulation = ee.Number(populationStats.get('b1')).round();\n\n// ----------- buildings footprint -------------\nvar buildingFootprintArea = buildingsFootprint.multiply(ee.Image.pixelArea());\nvar buildingFootprintAreaSum = buildingFootprintArea.reduceRegion({\n  reducer: ee.Reducer.sum(),\n  geometry: aoi,\n  scale: 10, \n  maxPixels: 1e13\n});\n\nvar buildingAreaHectares = ee.Number(buildingFootprintAreaSum.get('b1')).divide(10000).round();\n\n\n//  -------------- output ------------\nprint(croplandAreaHectares);\nprint(buildlandAreaHectares);\nprint(affectedPopulation);\nprint(buildingAreaHectares);\n\n\n\n\nCreate a map instance and Set the center point and zoom level of the map\n\nvar map = ui.Map();\nmap.centerObject(aoi,10.5);\n\nDisplay results The results of the data volt analysis are displayed in the lower right corner of the user interface. This user interface features a panel with a series of informational sections and a legend, organized as follows:\n\nResults Section: At the top of the panel, there is a “Results” header indicating the category of information provided. Below the header, several pieces of information are listed: - Flood Status: Dates between which the flood status is reported (2023-09-23 to 2023-09-30). - Estimated Flood Extent: The area affected by the flood, as estimated from Sentinel-1 imagery, is listed as 1023443 hectares. - Damaged Built-Up Areas: An estimate of the damaged urban area is provided (2943 hectares). - Damaged Crop Land Areas: The extent of the damage to cropland is reported (142837 hectares). - Estimated Damaged Buildings Land Area: This section lists the estimated area of damaged buildings (367 hectares). - Estimated Affected Population: The number of people estimated to be affected by the event (200094).\nLegend: Below the results and separated by a horizontal line, there is a legend that visually correlates colors with types of areas or statuses: - A blue square represents areas affected by the flood. - A red square denotes built-up areas. - A purple square indicates cropland. - A darker purple square is used for damaged buildings.\nEach legend item consists of a colored square followed by a text label that describes what the color represents in the context of the map being displayed.\n// Create a vertical panel that acts as a container for the entire right panel\nvar rightPanel = ui.Panel({\n  style: {\n    position: 'bottom-right',\n    padding: '8px 15px',\n    width: '250px' // Adjust the width as needed\n  },\n  layout: ui.Panel.Layout.flow('vertical') // Set the panel vertical layout\n});\n\n// Create a panel to display the results\nvar results = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0 0 8px 0' // Add a bottom margin to separate it from the legend panel\n  }\n});\n\n// Prepare the visualization parameters of the labels\n// Defines visual style parameters for text labels\nvar textVis = {\n  'margin':'0px 8px 2px 0px',\n  'fontWeight':'bold'\n};\nvar numberVIS = {\n  'margin':'0px 0px 15px 0px', \n  'color':'bf0f19',\n  'fontWeight':'bold'\n};\nvar subTextVis = {\n  'margin':'0px 0px 2px 0px',\n  'fontSize':'12px',\n  'color':'grey'\n};\n\nvar titleTextVis = {\n  'margin':'0px 0px 15px 0px',\n  'fontSize': '18px', \n  'color': '3333ff'\n};\n\n// Create text labels for titles and data\nvar title = ui.Label('Results', titleTextVis);\nvar text1 = ui.Label('Flood status between:', textVis);\nvar number1 = ui.Label(after_start.concat(\" and \",after_end), numberVIS);\n\n// The default text label is \"Please wait...\" Then replace it with the actual data\nvar text2 = ui.Label('Estimated flood extent:', textVis);\nvar text2_2 = ui.Label('Please wait...', subTextVis);\ndates(after_collection).evaluate(function(val){text2_2.setValue('based on Sentinel-1 imagery '+val)});\nvar number2 = ui.Label('Please wait...', numberVIS); \nflood_area_ha.evaluate(function(val){number2.setValue(val+' hectares')});\n\n//some more data\nvar text3 = ui.Label('Damaged Built-Up Areas (Ha):', textVis);\nvar number3 = ui.Label('Please wait...', numberVIS);\ndamagedBuildUpAreas.evaluate(function(val) {\n  number3.setValue(val + ' hectares');\n});\n\nvar text4 = ui.Label('Damaged Crop Land Areas (Ha):', textVis);\nvar number4 = ui.Label('Please wait...', numberVIS);\ndamagedCropLandAreas.evaluate(function(val) {\n  number4.setValue(val + ' hectares');\n});\n\nvar text5 = ui.Label('Estimated Damaged Buildings Land Area (Ha):', textVis);\nvar number5 = ui.Label('Please wait...', numberVIS);\nestimatedDamagedBuildings.evaluate(function(val) {\n  number5.setValue(val + ' hectares');\n});\n\nvar text6 = ui.Label('Estimated Affected Population:', textVis);\nvar number6 = ui.Label('Please wait...', numberVIS);\nestimatedAffectedPopulation.evaluate(function(val) {\n  number6.setValue(val.toString());\n});\n\nresults.add(ui.Panel([\n        title,\n        text1,\n        number1,\n        text2,\n        text2_2,\n        number2,\n        text3,\n        number3,\n        text4,\n        number4,\n        text5,\n        number5,\n        text6,\n        number6,\n       ]\n      ));\n      \n      \n// Create a function to generate legend items with colors and labels\nfunction createLegendItem(color, label) {\n  return ui.Panel({\n    widgets: [\n      ui.Label('', {\n        backgroundColor: color,\n        padding: '8px', // Adjust to fit the size of the legend color block\n        margin: '0 4px 0 0',\n      }),\n      ui.Label(label, {\n        margin: '-8px 0 0 4px',\n        padding: '8px 0px', // Adjust the upper and lower margins of the text to align it vertically\n        fontSize: '12px'\n      })\n    ],\n    layout: ui.Panel.Layout.Flow('horizontal')\n  });\n}\n\n\n// Create a legend panel and add a title\nvar legend = ui.Panel({\n  style: {\n    padding: '8px',\n    margin: '0'\n  }\n});\nlegend.add(ui.Label('Legend', {fontWeight: 'bold', fontSize: '16px', margin: '0 0 4px 0'}));\n\n// Create a legend item using a function and add it to the legend panel\nlegend.add(createLegendItem('blue', 'Flood'));\nlegend.add(createLegendItem('#fa0000', 'Built-up'));\nlegend.add(createLegendItem('#f096ff', 'CropLand'));\nlegend.add(createLegendItem('purple', 'Damaged Building'));\n\n// Add a splitter between the results panel and the Legend panel\nvar separatorLine = ui.Panel({\n  style: {\n    height: '2px',\n    backgroundColor: 'black',\n    margin: '8px 0'\n  }\n});\n\n// Add the results panel and Legend panel to the right panel container\nrightPanel.add(results);\nrightPanel.add(separatorLine); \nrightPanel.add(legend);\n\n// Adds the entire right panel to the map interface\nMap.add(rightPanel);"
  },
  {
    "objectID": "index.html#link-to-this-project",
    "href": "index.html#link-to-this-project",
    "title": "CASA00025 Group Project: Al Marj Flood Impact Comprehensive Evaluation System",
    "section": "",
    "text": "https://github.com/MengyuanHan1/BigData/tree/main"
  }
]